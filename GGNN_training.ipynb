{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8911ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import optuna.trial\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from qsprpred.models.assessment.methods import ModelAssessor\n",
    "from qsprpred.data import QSPRDataset\n",
    "from qsprpred.logs import logger\n",
    "from qsprpred.models.model import QSPRModel\n",
    "from qsprpred.models.monitors import BaseMonitor, HyperparameterOptimizationMonitor\n",
    "from qsprpred.models.hyperparam_optimization import HyperparameterOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09fdda4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to find the pandas get_adjustment() function to patch\n",
      "Failed to patch pandas - PandasTools will have limited functionality\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'qsprpred.extra.gpu.models.gdnn' from '/home/brokesm/anaconda3/lib/python3.12/site-packages/qsprpred/extra/gpu/models/gdnn.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import pandas as pd \n",
    "import json\n",
    "from qsprpred.data import QSPRDataset, RandomSplit\n",
    "from qsprpred.models import QSPRModel\n",
    "from qsprpred.data.descriptors.fingerprints import MorganFP\n",
    "from qsprpred.data.descriptors.sets import SmilesDesc\n",
    "from qsprpred.models import OptunaOptimization, TestSetAssessor, CrossValAssessor, SklearnModel\n",
    "from qsprpred.data.sampling.splits import DataSplit\n",
    "from qsprpred.data.processing.data_filters import RepeatsFilter, CategoryFilter\n",
    "from qsprpred.models import EarlyStoppingMode\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from qsprpred.extra.gpu.models.chemprop import ChempropModel\n",
    "\n",
    "from qsprpred.extra.gpu.models.dnn import DNNModel\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "from typing import Literal\n",
    "import sys\n",
    "#sys.path.insert(0, '/home/ubuntu/implementation/QSPRpred')\n",
    "\n",
    "#import qsprpred.extra.gpu.models.gdnn as gdnn_module\n",
    "#from importlib import reload\n",
    "#\n",
    "#reload(gdnn_module)\n",
    "\n",
    "modname = 'qsprpred.extra.gpu.models.gdnn'\n",
    "if modname in sys.modules:\n",
    "    del sys.modules[modname]\n",
    "\n",
    "import qsprpred.extra.gpu.models.gdnn as gdnn_module\n",
    "from qsprpred.extra.gpu.models.gdnn import GGNN\n",
    "importlib.reload(gdnn_module)\n",
    "\n",
    "#from qsprpred.extra.gpu.models.gdnn import DNNModel, GGNN\n",
    "#print(DNNModel.__init__.__code__.co_varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac295870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a folder structure\n",
    "\n",
    "os.makedirs(\"./output/models\", exist_ok=True)\n",
    "os.makedirs(\"./output/benchmarking/data\", exist_ok=True)\n",
    "os.makedirs(\"./output/optimization/data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c0a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a customsplit class\n",
    "# inherits from datasplit\n",
    "# input - QSPRDataset ids\n",
    "# output - (train,test) splits\n",
    "\n",
    "class CustomSplit(DataSplit):\n",
    "\n",
    "    def __init__(self, test_ids: list[list[str]]):\n",
    "        super().__init__()\n",
    "        self.test_ids = test_ids\n",
    "\n",
    "    def split(\n",
    "        self,\n",
    "        X: np.ndarray | pd.DataFrame, \n",
    "        y: np.ndarray | pd.DataFrame | pd.Series\n",
    "    ) -> Iterable[tuple[list[int], list[int]]]:\n",
    "        \"\"\"Uses only the specified IDs from the data set as test set\n",
    "        Returns an iterator of training and test split indices, \n",
    "        just like a scikit learn splitter would.\n",
    "        \"\"\"\n",
    "        splits = []\n",
    "        for test_ids in self.test_ids:\n",
    "            test = np.where(X.index.isin(test_ids))[0]\n",
    "            train = np.where(~X.index.isin(test_ids))[0]\n",
    "            splits.append((train, test))\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7065586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_ids(dataset_name, keep_ids):\n",
    "    return [f\"{dataset_name}_{\"0\" * (4 - len(str(id)))}{id}\" for id in keep_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046e884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(\n",
    "        target:Literal[\"P00918\",\"P03372\",\"P04637\",\"P08684\",\"P14416\",\"P22303\",\"P42336\",\"Q12809\",\"Q16637\",\"Q9Y468\"], \n",
    "        purpose:Literal[\"ForOptimization\",\"ForBenchmarking\"],\n",
    "        model:QSPRModel | None = None,\n",
    "        save = True\n",
    "        ) -> Tuple[QSPRDataset, List, List, List]:\n",
    "\n",
    "    dataset_name = f\"{purpose}_{target}\"\n",
    "    store_dir = f\"./output/{purpose[3:].lower()}/data\"\n",
    "\n",
    "    dataset = QSPRDataset.fromTableFile(\n",
    "        filename=f\"./papyrus_datasets/{target}.csv\",\n",
    "        sep=\",\",\n",
    "        store_dir=store_dir,\n",
    "        name=dataset_name,\n",
    "        target_props=[{\"name\": \"Y\", \"task\": \"SINGLECLASS\", \"th\":\"precomputed\"}],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    if model is not None:\n",
    "        if model.supportsEarlyStopping:\n",
    "            # In case of GNNs (both support early stopping) add SmilesDesc as descriptors\n",
    "            dataset.addDescriptors([SmilesDesc()])\n",
    "        else:\n",
    "            # In case of XGB (doesn't support early stopping) add MorganFP with default parameters as descriptors\n",
    "            dataset.addDescriptors([MorganFP()])\n",
    "\n",
    "    if save:\n",
    "        dataset.save()\n",
    "    \n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40be0a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'y']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([\"x\",\"y\",\"x\"]).tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaOptimization(HyperparameterOptimization):\n",
    "    \"\"\"Class for hyperparameter optimization of QSPRModels using Optuna.\n",
    "\n",
    "    Attributes:\n",
    "        nTrials (int):\n",
    "            number of trials for bayes optimization\n",
    "        nJobs (int):\n",
    "            number of jobs to run in parallel. At the moment only n_jobs=1 is supported.\n",
    "        bestScore (float):\n",
    "            best score found during optimization\n",
    "        bestParams (dict):\n",
    "            best parameters found during optimization\n",
    "\n",
    "    Example of OptunaOptimization for scikit-learn's MLPClassifier:\n",
    "        >>> model = SklearnModel(base_dir=\".\",\n",
    "        >>>                     alg = MLPClassifier(), alg_name=\"MLP\")\n",
    "        >>> search_space = {\n",
    "        >>>    \"learning_rate_init\": [\"float\", 1e-5, 1e-3,],\n",
    "        >>>    \"power_t\" : [\"discrete_uniform\", 0.2, 0.8, 0.1],\n",
    "        >>>    \"momentum\": [\"float\", 0.0, 1.0],\n",
    "        >>> }\n",
    "        >>> optimizer = OptunaOptimization(\n",
    "        >>>     scoring=\"average_precision\",\n",
    "        >>>     param_grid=search_space,\n",
    "        >>>     n_trials=10\n",
    "        >>> )\n",
    "        >>> best_params = optimizer.optimize(model, dataset) # dataset is a QSPRDataset\n",
    "\n",
    "    Available suggestion types:\n",
    "        [\"categorical\", \"discrete_uniform\", \"float\", \"int\", \"loguniform\", \"uniform\"]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        param_grid: dict,\n",
    "        model_assessor: ModelAssessor,\n",
    "        score_aggregation: Callable[[Iterable], float] = np.mean,\n",
    "        monitor: HyperparameterOptimizationMonitor | None = None,\n",
    "        n_trials: int = 100,\n",
    "        n_jobs: int = 1,\n",
    "    ):\n",
    "        \"\"\"Initialize the class for hyperparameter optimization\n",
    "        of QSPRModels using Optuna.\n",
    "\n",
    "        Args:\n",
    "            param_grid (dict):\n",
    "                search space for bayesian optimization, keys are the parameter names,\n",
    "                values are lists with first element the type of the parameter and the\n",
    "                following elements the parameter bounds or values.\n",
    "            model_assessor (ModelAssessor):\n",
    "                assessment method to use for the optimization\n",
    "                (default: CrossValAssessor)\n",
    "            score_aggregation (Callable):\n",
    "                function to aggregate the scores of different folds if the assessment\n",
    "                method returns multiple predictions\n",
    "            monitor (HyperparameterOptimizationMonitor):\n",
    "                monitor for the optimization, if None, a BaseMonitor is used\n",
    "            n_trials (int):\n",
    "                number of trials for bayes optimization\n",
    "            n_jobs (int):\n",
    "                number of jobs to run in parallel.\n",
    "                At the moment only n_jobs=1 is supported.\n",
    "        \"\"\"\n",
    "        super().__init__(param_grid, model_assessor, score_aggregation, monitor)\n",
    "        if monitor is None:\n",
    "            self.monitor = BaseMonitor()\n",
    "        search_space_types = [\n",
    "            \"categorical\",\n",
    "            \"discrete_uniform\",\n",
    "            \"float\",\n",
    "            \"int\",\n",
    "            \"loguniform\",\n",
    "            \"uniform\",\n",
    "        ]\n",
    "        if not all(v[0] in search_space_types for v in param_grid.values()):\n",
    "            logger.error(\n",
    "                f\"Search space {param_grid} is missing or has invalid search type(s), \"\n",
    "                \"see OptunaOptimization docstring for example.\"\n",
    "            )\n",
    "            raise ValueError(\n",
    "                \"Search space for optuna optimization is missing or \"\n",
    "                \"has invalid search type(s).\"\n",
    "            )\n",
    "\n",
    "        self.nTrials = n_trials\n",
    "        self.nJobs = n_jobs\n",
    "        if self.nJobs > 1:\n",
    "            logger.warning(\n",
    "                \"At the moment n_jobs>1 not available for bayes optimization, \"\n",
    "                \"n_jobs set to 1.\"\n",
    "            )\n",
    "            self.nJobs = 1\n",
    "        self.bestScore = -np.inf\n",
    "        self.bestParams = None\n",
    "        self.config.update(\n",
    "            {\n",
    "                \"n_trials\": n_trials,\n",
    "                \"n_jobs\": n_jobs,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        model: QSPRModel,\n",
    "        ds: QSPRDataset,\n",
    "        save_params: bool = True,\n",
    "        refit_optimal: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> dict:\n",
    "        \"\"\"Bayesian optimization of hyperparameters using optuna.\n",
    "\n",
    "        Args:\n",
    "            model (QSPRModel): the model to optimize\n",
    "            ds (QSPRDataset): dataset to use for the optimization\n",
    "            save_params (bool):\n",
    "                whether to set and save the best parameters to the model\n",
    "                after optimization\n",
    "            refit_optimal (bool):\n",
    "                Whether to refit the model with the optimal parameters on the\n",
    "                entire training set after optimization. This implies 'save_params=True'.\n",
    "            **kwargs: additional arguments for the assessment method\n",
    "\n",
    "        Returns:\n",
    "            dict: best parameters found during optimization\n",
    "        \"\"\"\n",
    "        import optuna\n",
    "\n",
    "        self.monitor.onOptimizationStart(\n",
    "            model, ds, self.config, self.__class__.__name__\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            \"Bayesian optimization can take a while \"\n",
    "            \"for some hyperparameter combinations\"\n",
    "        )\n",
    "        # create optuna study\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            sampler=optuna.samplers.TPESampler(seed=model.randomState),\n",
    "        )\n",
    "        logger.info(\n",
    "            \"Bayesian optimization started: %s\"\n",
    "            % datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "        study.optimize(\n",
    "            lambda t: self.objective(t, model, ds), self.nTrials, n_jobs=self.nJobs\n",
    "        )\n",
    "        logger.info(\n",
    "            \"Bayesian optimization ended: %s\"\n",
    "            % datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "        # save the best study\n",
    "        trial = study.best_trial\n",
    "        trials = study.get_trials()\n",
    "        top_n = sorted(trials, key=lambda x:x.values)[:10]\n",
    "        \n",
    "        aggr_values = {k:[] for k in top_n[0].params.keys()}\n",
    "        for tr in top_n:\n",
    "            params = tr.params\n",
    "            for k,v in params.items():\n",
    "                aggr_values[k].append(v)\n",
    "\n",
    "        next_iter = {\n",
    "            k:[self.config[\"param_grid\"][k][0], np.min(v), np.max(v)] if self.config[\"param_grid\"][k][0] != \"categorical\" \\\n",
    "            else [\"categorical\", np.unique(v).tolist()] \\\n",
    "            for k,v in aggr_values.items()\n",
    "        }\n",
    "\n",
    "        # log the best study\n",
    "        logger.info(\"Bayesian optimization best params: %s\" % trial.params)\n",
    "        # save the best score and parameters, return the best parameters\n",
    "        self.bestScore = trial.value\n",
    "        self.bestParams = trial.params\n",
    "\n",
    "        self.monitor.onOptimizationEnd(self.bestScore, self.bestParams)\n",
    "        # save the best parameters to the model if requested\n",
    "        self.saveResults(model, ds, save_params, refit_optimal)\n",
    "        # return self.bestParams\n",
    "        return next_iter\n",
    "    \n",
    "    def objective(\n",
    "        self, trial: optuna.trial.Trial, model: QSPRModel, ds: QSPRDataset, **kwargs\n",
    "    ) -> float:\n",
    "        \"\"\"Objective for bayesian optimization.\n",
    "\n",
    "        Arguments:\n",
    "            trial (optuna.trial.Trial): trial object for the optimization\n",
    "            model (QSPRModel): the model to optimize\n",
    "            ds (QSPRDataset): dataset to use for the optimization\n",
    "            **kwargs: additional arguments for the assessment method\n",
    "\n",
    "        Returns:\n",
    "            float: score of the model with the current parameters\n",
    "        \"\"\"\n",
    "        bayesian_params = {}\n",
    "        # get the suggested parameters for the current trial\n",
    "        for key, value in self.paramGrid.items():\n",
    "            if value[0] == \"categorical\":\n",
    "                bayesian_params[key] = trial.suggest_categorical(key, value[1])\n",
    "            elif value[0] == \"discrete_uniform\":\n",
    "                bayesian_params[key] = trial.suggest_float(\n",
    "                    key, value[1], value[2], step=value[3]\n",
    "                )\n",
    "            elif value[0] == \"float\":\n",
    "                bayesian_params[key] = trial.suggest_float(key, value[1], value[2])\n",
    "            elif value[0] == \"int\":\n",
    "                bayesian_params[key] = trial.suggest_int(key, value[1], value[2])\n",
    "            elif value[0] == \"loguniform\":\n",
    "                bayesian_params[key] = trial.suggest_float(\n",
    "                    key, value[1], value[2], log=True\n",
    "                )\n",
    "            elif value[0] == \"uniform\":\n",
    "                bayesian_params[key] = trial.suggest_float(key, value[1], value[2])\n",
    "        self.monitor.onIterationStart(bayesian_params)\n",
    "        # assess the model with the current parameters and return the score\n",
    "        scores = self.runAssessment(\n",
    "            model,\n",
    "            ds=ds,\n",
    "            save=False,\n",
    "            parameters=bayesian_params,\n",
    "            monitor=self.monitor,\n",
    "            **kwargs,\n",
    "        )\n",
    "        score = self.scoreAggregation(scores)\n",
    "        logger.info(bayesian_params)\n",
    "        logger.info(f\"Score: {score}, std: {np.std(scores)}\")\n",
    "        self.monitor.onIterationEnd(score, list(scores))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8956aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-07 09:20:27,245] A new study created in memory with name: no-name-a709f69c-0a9e-4dfe-b981-cc633e77e08e\n",
      "[I 2025-12-07 09:20:27,249] Trial 0 finished with value: 0.6826095879557307 and parameters: {'x': -0.8262019050787348}. Best is trial 0 with value: 0.6826095879557307.\n",
      "[I 2025-12-07 09:20:27,252] Trial 1 finished with value: 0.8902280868283088 and parameters: {'x': 0.9435189912388138}. Best is trial 0 with value: 0.6826095879557307.\n",
      "[I 2025-12-07 09:20:27,255] Trial 2 finished with value: 1.1463660161770181e-06 and parameters: {'x': -0.0010706848351298426}. Best is trial 2 with value: 1.1463660161770181e-06.\n",
      "[I 2025-12-07 09:20:27,257] Trial 3 finished with value: 0.3141936180281509 and parameters: {'x': -0.5605297655148662}. Best is trial 2 with value: 1.1463660161770181e-06.\n",
      "[I 2025-12-07 09:20:27,258] Trial 4 finished with value: 0.23963530551946446 and parameters: {'x': -0.4895255923028585}. Best is trial 2 with value: 1.1463660161770181e-06.\n",
      "[I 2025-12-07 09:20:27,259] Trial 5 finished with value: 0.4302948257013252 and parameters: {'x': 0.6559686163996912}. Best is trial 2 with value: 1.1463660161770181e-06.\n",
      "[I 2025-12-07 09:20:27,260] Trial 6 finished with value: 0.2684203889121541 and parameters: {'x': -0.5180930311364496}. Best is trial 2 with value: 1.1463660161770181e-06.\n",
      "[I 2025-12-07 09:20:27,263] Trial 7 finished with value: 0.10486909076884882 and parameters: {'x': 0.3238349745917646}. Best is trial 2 with value: 1.1463660161770181e-06.\n",
      "[I 2025-12-07 09:20:27,265] Trial 8 finished with value: 0.12050001439071796 and parameters: {'x': -0.34713111988226864}. Best is trial 2 with value: 1.1463660161770181e-06.\n",
      "[I 2025-12-07 09:20:27,267] Trial 9 finished with value: 0.7557520421719438 and parameters: {'x': 0.8693400037798467}. Best is trial 2 with value: 1.1463660161770181e-06.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -1, 1)\n",
    "    return x**2\n",
    "\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "trials = study.get_trials()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9bb6bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FrozenTrial(number=2, state=<TrialState.COMPLETE: 1>, values=[1.1463660161770181e-06], datetime_start=datetime.datetime(2025, 12, 7, 9, 20, 27, 253112), datetime_complete=datetime.datetime(2025, 12, 7, 9, 20, 27, 253885), params={'x': -0.0010706848351298426}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'x': FloatDistribution(high=1.0, log=False, low=-1.0, step=None)}, trial_id=2, value=None),\n",
       " FrozenTrial(number=7, state=<TrialState.COMPLETE: 1>, values=[0.10486909076884882], datetime_start=datetime.datetime(2025, 12, 7, 9, 20, 27, 263159), datetime_complete=datetime.datetime(2025, 12, 7, 9, 20, 27, 263718), params={'x': 0.3238349745917646}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'x': FloatDistribution(high=1.0, log=False, low=-1.0, step=None)}, trial_id=7, value=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(trials,key=lambda x: x.values)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(\n",
    "        model:QSPRModel, \n",
    "        dataset:QSPRDataset, \n",
    "        search_space:dict, \n",
    "        scoring:str, \n",
    "        val_ids:List,\n",
    "        test_ids:List\n",
    "        ):\n",
    "    # opravit, prvy split CVA je rozdelenie povodneho datasetu na train/test\n",
    "    # nasledny dataset split bude na train/val (val set je iba na early stopping v ramci CVA)\n",
    "    # Uz asi hotovo\n",
    "    gridsearcher = OptunaOptimization(\n",
    "        n_trials=100,\n",
    "        param_grid=search_space,\n",
    "        model_assessor=CrossValAssessor(scoring=scoring, split=CustomSplit([test_ids])),\n",
    "    )\n",
    "\n",
    "    dataset.prepareDataset(\n",
    "        split=CustomSplit([val_ids])\n",
    "    )\n",
    "    gridsearcher.optimize(model, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f92aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qsprpred - WARNING - Random state supplied, but alg <class 'qsprpred.extra.gpu.models.gdnn.GGNN'> does not support it. Ignoring this setting.\n"
     ]
    }
   ],
   "source": [
    "model_ggnn = gdnn_module.DNNModel(\n",
    "    base_dir='./output/models/GGNN',\n",
    "    name='GGNNModel',\n",
    "    parameters={'n_epochs': 100,\n",
    "                'out_feats': 74,          # 74-256 for example\n",
    "                'in_feats': 74,\n",
    "                'steps': 3,\n",
    "                'n_hidden_layers': 2,\n",
    "                'dropout_rate': 0.2,\n",
    "                \"optim_lr\":1e-4,\n",
    "                \"batch_size\":128,\n",
    "               },\n",
    "    tol=0.01,\n",
    "    random_state=42,\n",
    "    patience=50\n",
    ")\n",
    "\n",
    "search_space_ggnn = {\n",
    "    \"n_hidden_layers\": [\"int\", 1, 6],\n",
    "    \"dropout_rate\": [\"float\", 0.05, 0.5],\n",
    "    \"steps\": [\"int\", 2, 5],\n",
    "    \"batch_size\": [\"categorical\", [32,64,128,256]],\n",
    "    'out_feats': [\"int\",74,256],\n",
    "    'in_feats': [\"int\",74,74],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chemprop = ChempropModel(\n",
    "    base_dir='./output/models/Chemprop',\n",
    "    name='ChempropModel',\n",
    "    parameters={\n",
    "        \"epochs\": 5,\n",
    "        \"loss_function\":'binary_cross_entropy'\n",
    "        },\n",
    "    quiet_logger=False\n",
    ")\n",
    "\n",
    "search_space_chemprop = {\n",
    "    \"epochs\": [\"int\", 1,5],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b22915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = SklearnModel(\n",
    "            name=\"XGBModel\",\n",
    "            alg=GradientBoostingClassifier,\n",
    "            base_dir=\"./output/models/XGB\",\n",
    "            parameters={\n",
    "                \"max_depth\":2,\n",
    "                \"n_estimators\":10\n",
    "            }\n",
    "        )\n",
    "\n",
    "search_space_xgb = {\n",
    "    \"max_depth\": [\"int\", 2, 10],\n",
    "    \"n_estimators\": [\"int\", 5,500]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3211fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_loader(\n",
    "        target:Literal[\"P00918\",\"P03372\",\"P04637\",\"P08684\",\"P14416\",\"P22303\",\"P42336\",\"Q12809\",\"Q16637\",\"Q9Y468\"], \n",
    "        split_type:Literal[\"random\", \"cluster\", \"aggregate_cluster\"],\n",
    "        seed,\n",
    "        purpose:Literal[\"ForBenchmarking\",\"ForOptimization\"]\n",
    "        ):\n",
    "    seed = str(seed)\n",
    "\n",
    "    with open(f\"./papyrus_datasets/{split_type}_split.json\") as file:\n",
    "        json_file = file.read()\n",
    "\n",
    "    split = json.loads(json_file)\n",
    "    \n",
    "    train_ids = split[split_type][target][seed][\"train\"]\n",
    "    val_ids = split[split_type][target][seed][\"valid\"]\n",
    "    test_ids = split[split_type][target][seed][\"test\"]\n",
    "\n",
    "    train_ids = select_ids(f\"{purpose}_{target}\",list(train_ids))\n",
    "    val_ids = select_ids(f\"{purpose}_{target}\",list(val_ids))\n",
    "    test_ids = select_ids(f\"{purpose}_{target}\",list(test_ids))\n",
    "\n",
    "    return train_ids, val_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e77e3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(\n",
    "        target:Literal[\"P00918\",\"P03372\",\"P04637\",\"P08684\",\"P14416\",\"P22303\",\"P42336\",\"Q12809\",\"Q16637\",\"Q9Y468\"],\n",
    "        split:Literal[\"random\",\"cluster\",\"aggregate_cluster\"],\n",
    "        model:QSPRModel,\n",
    "        search_space:dict,\n",
    "        seed = 0\n",
    "        ):\n",
    "\n",
    "        dataset = data_loading(target,model=model, purpose=\"ForOptimization\")\n",
    "        train_ids, val_ids, test_ids = set_loader(target,split,seed,purpose=\"ForOptimization\")\n",
    "        #selected_ids = train_ids + val_ids\n",
    "        #dataset.prepareDataset(data_filters=[CategoryFilter(name=\"QSPRID\", values=selected_ids, keep=True)])\n",
    "        \n",
    "        hyperparameter_optimization(model=model, dataset=dataset, search_space=search_space, scoring=\"matthews_corrcoef\", val_ids=val_ids, test_ids=test_ids)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50630b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no_cuda': 'Turn off cuda (i.e., use CPU instead of GPU).',\n",
       " 'gpu': 'Which GPU to use.',\n",
       " 'num_workers': 'Number of workers for the parallel data loading (0 means sequential).',\n",
       " 'batch_size': 'Batch size.',\n",
       " 'no_cache_mol': 'Whether to not cache the RDKit molecule for each SMILES string to reduce memory usage (cached by default).',\n",
       " 'empty_cache': 'Whether to empty all caches before training or predicting. This is necessary if multiple jobs are run within a single script and the atom or bond features change.',\n",
       " 'loss_function': 'Choice of loss function. Loss functions are limited to compatible dataset types.',\n",
       " 'metric': \"Metric to use with the validation set for early stopping. Defaults to 'auc' for classification, 'rmse' for regression. Note. In Chemprop this metric is also used for test-set evaluation, but in QSPRpred this is determined by the scoring parameter in assessment.\",\n",
       " 'bias': 'Whether to add bias to linear layers.',\n",
       " 'hidden_size': 'Dimensionality of hidden layers in MPN.',\n",
       " 'depth': 'Number of message passing steps.',\n",
       " 'mpn_shared': \"Whether to use the same message passing neural network for all input molecule Only relevant if 'number_of_molecules > 1'\",\n",
       " 'dropout': 'Dropout probability.',\n",
       " 'activation': 'Activation function.',\n",
       " 'atom_messages': 'Centers messages on atoms instead of on bonds.',\n",
       " 'undirected': 'Undirected edges (always sum the two relevant bond vectors).',\n",
       " 'ffn_hidden_size': 'Hidden dim for higher-capacity FFN (defaults to hidden_size).',\n",
       " 'ffn_num_layers': 'Number of layers in FFN after MPN encoding.',\n",
       " 'epochs': 'Number of epochs to run.',\n",
       " 'warmup_epochs': \"Number of epochs during which learning rate increases linearly from 'init_lr' to 'max_lr'. Afterwards, learning rate decreases exponentially from 'max_lr' to 'final_lr'.\",\n",
       " 'init_lr': 'Initial learning rate.',\n",
       " 'max_lr': 'Maximum learning rate.',\n",
       " 'final_lr': 'Final learning rate.',\n",
       " 'grad_clip': 'Maximum magnitude of gradient during training.',\n",
       " 'class_balance': 'Trains with an equal number of positives and negatives in each batch.',\n",
       " 'evidential_regularization': 'Value used in regularization for evidential loss function. The default value recommended by Soleimany et al.(2021) is 0.2. Optimal value is dataset-dependent; it is recommended that users test different values to find the best value for their model.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChempropModel.getAvailableParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"P00918\",\"P03372\",\"P04637\",\"P08684\",\"P14416\",\"P22303\",\"P42336\",\"Q12809\",\"Q16637\",\"Q9Y468\"]\n",
    "splits = [\"random\",\"cluster\", \"aggregate_cluster\"]\n",
    "models = [model_xgb, model_ggnn, model_chemprop]\n",
    "search_spaces = [search_space_xgb, search_space_ggnn, search_space_chemprop]\n",
    "\n",
    "for target in targets:\n",
    "    for split in splits:\n",
    "        for model, search_space in zip(models, search_spaces):\n",
    "            model.name += f\"_{target}_{split}\"\n",
    "            optimize(target = target, split = split, model = model, search_space = search_space)\n",
    "            model.name = model.name.split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eedd70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(\n",
    "        target:Literal[\"P00918\",\"P03372\",\"P04637\",\"P08684\",\"P14416\",\"P22303\",\"P42336\",\"Q12809\",\"Q16637\",\"Q9Y468\"], \n",
    "        split_type:Literal[\"random\",\"cluster\",\"aggregate_cluster\"],\n",
    "        model:Literal[\"XGB\",\"GGNN\",\"Chemprop\"]\n",
    "    ):\n",
    "\n",
    "    with open(f\"./output/models/{model}/{model}Model_{target}_{split_type}/{model}Model_{target}_{split_type}_meta.json\") as f:\n",
    "        params = f.read()\n",
    "\n",
    "    params = json.loads(params)\n",
    "    return params[\"py/state\"][\"parameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d00218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_benchmarking(dataset:QSPRDataset,descriptors, chemprop=False):\n",
    "\n",
    "    dataset.addDescriptors([descriptors])\n",
    "\n",
    "    if chemprop:\n",
    "        # binary cross entropy loss cannot deal with target variable being of type int\n",
    "        dataset.transformProperties([\"Y\",\"Y_original\"],transformer=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68b45a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(\n",
    "    target:Literal[\"P00918\",\"P03372\",\"P04637\",\"P08684\",\"P14416\",\"P22303\",\"P42336\",\"Q12809\",\"Q16637\",\"Q9Y468\"],\n",
    "    split_type:Literal[\"random\",\"cluster\",\"aggregate_cluster\"]\n",
    "):\n",
    "    os.makedirs(f\"./output/benchmarking/{target}/{split_type}\", exist_ok=True)\n",
    "\n",
    "    # save the dataset corresponding to a given target\n",
    "    data_loading(target,purpose=\"ForBenchmarking\")\n",
    "    \n",
    "    results = {\n",
    "        \"model\":[],\n",
    "        \"metric\":[],\n",
    "        \"score\":[]\n",
    "    }\n",
    "    for metric in [\"matthews_corrcoef\",\"f1\",\"recall\",\"precision\",\"roc_auc\"]:\n",
    "        for seed in range(1,21):\n",
    "            # get the ids for training, validation and test sets for a given combination of target + split + seed\n",
    "            _, val_ids, test_ids = set_loader(target,split_type,seed=seed, purpose=\"ForBenchmarking\")\n",
    "            dataset_path = f\"./output/benchmarking/data/ForBenchmarking_{target}/ForBenchmarking_{target}_meta.json\"\n",
    "            \n",
    "            dataset_xgb = QSPRDataset.fromFile(dataset_path)\n",
    "            dataset_ggnn = QSPRDataset.fromFile(dataset_path)\n",
    "            dataset_chemprop = QSPRDataset.fromFile(dataset_path)\n",
    "\n",
    "\n",
    "            prepare_for_benchmarking(dataset_xgb,MorganFP())\n",
    "            prepare_for_benchmarking(dataset_ggnn,SmilesDesc())\n",
    "            prepare_for_benchmarking(dataset_chemprop,SmilesDesc(), chemprop=True)\n",
    "\n",
    "            model_xgb.parameters = get_model_params(target,split_type,\"XGB\")\n",
    "            model_ggnn.parameters = get_model_params(target,split_type,\"GGNN\")\n",
    "            model_chemprop.parameters = get_model_params(target,split_type,\"Chemprop\")\n",
    "\n",
    "            proba = True\n",
    "            if metric == \"matthews_corrcoef\":\n",
    "                proba = False\n",
    "\n",
    "            dataset_xgb.prepareDataset(split = CustomSplit([test_ids]))\n",
    "            xgb_score = TestSetAssessor(scoring=metric, use_proba=proba)(model_xgb, dataset_xgb)\n",
    "            results[\"model\"].append(\"XGB\")\n",
    "            results[\"metric\"].append(metric)\n",
    "            results[\"score\"].append(xgb_score.item())\n",
    "\n",
    "\n",
    "            # Tu mozno pouzit iba CVA, kde na val mnozine najdem best epoch pomocou early stopping\n",
    "            # Na test mnozine v ramci toho isteho CVA vypocitam skore\n",
    "            # Uz opravene\n",
    "            ggnn_score = CrossValAssessor(\n",
    "                scoring=metric,\n",
    "                use_proba=proba,\n",
    "                mode=EarlyStoppingMode.RECORDING,\n",
    "                split=CustomSplit([test_ids]))(model_ggnn, dataset_ggnn,split=CustomSplit([val_ids]))\n",
    "            print(f\"Best epoch found for GGNN: {model_ggnn.earlyStopping.optimalEpochs}\")\n",
    "            results[\"model\"].append(\"GGNN\")\n",
    "            results[\"metric\"].append(metric)\n",
    "            results[\"score\"].append(ggnn_score.item())\n",
    "\n",
    "            chemprop_score = CrossValAssessor(\n",
    "                scoring=metric,\n",
    "                use_proba=proba,\n",
    "                mode=EarlyStoppingMode.RECORDING,\n",
    "                split=CustomSplit([test_ids]))(model_chemprop, dataset_chemprop,split=CustomSplit([test_ids]))\n",
    "            print(f\"Best epoch found for Chemprop: {model_chemprop.earlyStopping.optimalEpochs}\")\n",
    "            results[\"model\"].append(\"Chemprop\")\n",
    "            results[\"metric\"].append(metric)\n",
    "            results[\"score\"].append(chemprop_score.item())\n",
    "            \n",
    "    pd.DataFrame(results).to_csv(f\"./output/benchmarking/{target}/{split_type}/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383e426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Y</th>\n",
       "      <th>QSPRID</th>\n",
       "      <th>Y_original</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QSPRID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ChempropTutorialDataset_0000</th>\n",
       "      <td>CC(=O)OCC1OC(NS(=O)(=O)NO)C=CC1OC(C)=O</td>\n",
       "      <td>1</td>\n",
       "      <td>ChempropTutorialDataset_0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChempropTutorialDataset_0001</th>\n",
       "      <td>CC(=O)OCC1OC(CC(=O)C=Cc2cccc(O)c2)C(OC(C)=O)C(...</td>\n",
       "      <td>0</td>\n",
       "      <td>ChempropTutorialDataset_0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChempropTutorialDataset_0002</th>\n",
       "      <td>COc1ccc(CNc2ccc(S(N)(=O)=O)cc2)cc1</td>\n",
       "      <td>1</td>\n",
       "      <td>ChempropTutorialDataset_0002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChempropTutorialDataset_0003</th>\n",
       "      <td>CN(C)C=Nc1ncnc2c1ncn2CC(=O)Nc1ccc(S(N)(=O)=O)cc1</td>\n",
       "      <td>1</td>\n",
       "      <td>ChempropTutorialDataset_0003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChempropTutorialDataset_0004</th>\n",
       "      <td>Cc1sc(-c2noc(N)c2S(N)(=O)=O)cc1S(N)(=O)=O</td>\n",
       "      <td>1</td>\n",
       "      <td>ChempropTutorialDataset_0004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         SMILES  \\\n",
       "QSPRID                                                                            \n",
       "ChempropTutorialDataset_0000             CC(=O)OCC1OC(NS(=O)(=O)NO)C=CC1OC(C)=O   \n",
       "ChempropTutorialDataset_0001  CC(=O)OCC1OC(CC(=O)C=Cc2cccc(O)c2)C(OC(C)=O)C(...   \n",
       "ChempropTutorialDataset_0002                 COc1ccc(CNc2ccc(S(N)(=O)=O)cc2)cc1   \n",
       "ChempropTutorialDataset_0003   CN(C)C=Nc1ncnc2c1ncn2CC(=O)Nc1ccc(S(N)(=O)=O)cc1   \n",
       "ChempropTutorialDataset_0004          Cc1sc(-c2noc(N)c2S(N)(=O)=O)cc1S(N)(=O)=O   \n",
       "\n",
       "                              Y                        QSPRID  Y_original  \n",
       "QSPRID                                                                     \n",
       "ChempropTutorialDataset_0000  1  ChempropTutorialDataset_0000           1  \n",
       "ChempropTutorialDataset_0001  0  ChempropTutorialDataset_0001           0  \n",
       "ChempropTutorialDataset_0002  1  ChempropTutorialDataset_0002           1  \n",
       "ChempropTutorialDataset_0003  1  ChempropTutorialDataset_0003           1  \n",
       "ChempropTutorialDataset_0004  1  ChempropTutorialDataset_0004           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# from qsprpred.data import QSPRDataset, RandomSplit\n",
    "# from qsprpred.data.descriptors.fingerprints import MorganFP\n",
    "# from qsprpred.data.descriptors.sets import SmilesDesc\n",
    "\n",
    "# # Create dataset\n",
    "# dataset = QSPRDataset.fromTableFile(\n",
    "#     filename=\"./papyrus_datasets/P00918.csv\",\n",
    "#     sep=\",\",\n",
    "#     store_dir=\"./tutorial_output/data\",\n",
    "#     name=\"ChempropTutorialDataset\",\n",
    "#     target_props=[{\"name\": \"Y\", \"task\": \"SINGLECLASS\", \"th\":\"precomputed\"}],\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # calculate compound features and split dataset into train and test\n",
    "# feature_calculators = [SmilesDesc()]\n",
    "# dataset.prepareDataset(\n",
    "#     split=RandomSplit(test_fraction=0.2, dataset=dataset),\n",
    "#     feature_calculators=feature_calculators)\n",
    "\n",
    "# dataset.getDF().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c092ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qsprpred - WARNING - Existing data set found, but also found a data frame in store. Refusing to overwrite data. If you want to overwrite data in store, set overwrite=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGNN updated\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'out_feats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits:\n\u001b[0;32m----> 3\u001b[0m         benchmark(target,split)\n",
      "Cell \u001b[0;32mIn[20], line 48\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(target, split_type)\u001b[0m\n\u001b[1;32m     42\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(xgb_score\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Tu mozno pouzit iba CVA, kde na val mnozine najdem best epoch pomocou early stopping\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Na test mnozine v ramci toho isteho CVA vypocitam skore\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Uz opravene\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m ggnn_score \u001b[38;5;241m=\u001b[39m CrossValAssessor(\n\u001b[1;32m     49\u001b[0m     scoring\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m     50\u001b[0m     use_proba\u001b[38;5;241m=\u001b[39mproba,\n\u001b[1;32m     51\u001b[0m     mode\u001b[38;5;241m=\u001b[39mEarlyStoppingMode\u001b[38;5;241m.\u001b[39mRECORDING,\n\u001b[1;32m     52\u001b[0m     split\u001b[38;5;241m=\u001b[39mCustomSplit([test_ids]))(model_ggnn, dataset_ggnn,split\u001b[38;5;241m=\u001b[39mCustomSplit([val_ids]))\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest epoch found for GGNN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_ggnn\u001b[38;5;241m.\u001b[39mearlyStopping\u001b[38;5;241m.\u001b[39moptimalEpochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGGNN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/qsprpred/models/assessment/methods.py:211\u001b[0m, in \u001b[0;36mCrossValAssessor.__call__\u001b[0;34m(self, model, ds, save, parameters, monitor, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m monitor\u001b[38;5;241m.\u001b[39monFoldStart(\n\u001b[1;32m    208\u001b[0m     fold\u001b[38;5;241m=\u001b[39mi, X_train\u001b[38;5;241m=\u001b[39mX_train, y_train\u001b[38;5;241m=\u001b[39my_train, X_test\u001b[38;5;241m=\u001b[39mX_test, y_test\u001b[38;5;241m=\u001b[39my_test\n\u001b[1;32m    209\u001b[0m )\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# fit model\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m crossval_estimator \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloadEstimator(evalparams)\n\u001b[1;32m    212\u001b[0m model_fit \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    213\u001b[0m     X_train,\n\u001b[1;32m    214\u001b[0m     y_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    219\u001b[0m )\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# make predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/qsprpred/extra/gpu/models/gdnn.py:516\u001b[0m, in \u001b[0;36mDNNModel.loadEstimator\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUninitialized model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# initialize model - GGNN here\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malg(\n\u001b[1;32m    517\u001b[0m     n_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnDim,\n\u001b[1;32m    518\u001b[0m     n_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnClass,\n\u001b[1;32m    519\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    520\u001b[0m     gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpus,\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;66;03m#FIXED is_reg\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;66;03m#is_reg=False,#self.task == ModelTasks.REGRESSION,\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     is_reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m ModelTasks\u001b[38;5;241m.\u001b[39mREGRESSION,\n\u001b[1;32m    524\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatience,\n\u001b[1;32m    525\u001b[0m     tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;66;03m#FIXED don't accept any parameters\u001b[39;00m\n\u001b[1;32m    527\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    528\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# set parameters if available and return\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m#FIXED load parameters\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# new_parameters = self.getParameters(params)\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# if new_parameters is not None:\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m#    estimator.set_params(**new_parameters)\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/qsprpred/extra/gpu/models/gdnn.py:62\u001b[0m, in \u001b[0;36mGGNN.__init__\u001b[0;34m(self, n_dim, device, gpus, is_reg, patience, tol, parameters, n_class)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_steps\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_etypes \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_etypes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_feats \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_feats\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_lr \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptim__lr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_wd \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptim__weight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'out_feats'"
     ]
    }
   ],
   "source": [
    "for target in targets:\n",
    "    for split in splits:\n",
    "        benchmark(target,split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33bf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'C:\\\\Users\\\\marti\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
